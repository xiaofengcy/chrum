{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: black; font-family: 'courier new'; font-size: 1.2em\">\n",
    "\n",
    "# INTRODUCTION\n",
    "Challenge-1: Telecom Churn Predictive with Automated Data Science Techniques\n",
    "\n",
    "The goal of this challenge is to perform the telecom churn prediction as defined by the KDD-2009 challenge using supervised learning techniques. Here is a summary of tasks performed in this notebook:\n",
    "\n",
    "1. Parse the training data from provided files.\n",
    "2. Pre-process the data and and filter columns for feature selection and engineering.\n",
    "3. Apply feature selection rules shortisted in above step.\n",
    "4. Random under sampling to reduce samples of majority class.\n",
    "5. Train an AUOTML model with the under sampled dataset.\n",
    "\n",
    "Details of these tasks are in subsequent sections.\n",
    "\n",
    "For AUTOML, we have selected <b>auto-sklearn</b>, which is an automated machine learning toolkit developed as a wrapper\n",
    "around the scikit-learn estimators. More details can be found here - https://automl.github.io/auto-sklearn/stable/#\n",
    "\n",
    "## 1. Parsing\n",
    "This is done in two steps. In the first step, training data is pre-analyzed to find appropriate datatypes for the features. In the second parse, training data is combined into a pandas dataframe using the reduced datatypes. This drastically improves memory footprint of the training data.\n",
    "\n",
    "## 2. Feature selection\n",
    "### a. Categorical features\n",
    "We chose to remove all categorical features from the training data, i.e., the last 260 columns. \n",
    "<br>We tried to incorporate the categorical columns into different estimators by converting them to numerics (using hash buckets, one hot encoding etc.). However they only seemed to degrade the results.</br>\n",
    "\n",
    "### b. Null values\n",
    "The dataset is highly sparse and many columns have a very high volume of null values. We chose to use a null value threshold of 0.90. In other words, all columns having more than 90% of their values as null values were removed.\n",
    "\n",
    "### c. Null filler\n",
    "We chose to fill all remaining null values with 0. \n",
    "<br>We tried with other approaches such as ffill (forward fill), bfill (backward fill) and majority selection. However \"0\" filled null values seemd to give the best results.</br>\n",
    "\n",
    "### c. Variance\n",
    "We also chose to use a variance threshold of 0.9. In other words, columns whose values did not have a variance greater than 90% (compared to the standard deviation of the column), were removed.\n",
    "\n",
    "### d. Recursive feature elimination\n",
    "We chose to use recursive feature elimination (<b>RFECV</b>) schemes provided by scikit-learn libraries, to reduce feature dimensionality. We chose to use RandomForestClassifier as the estimator for RFECV with validation based on f1-weighted scores.\n",
    "\n",
    "Since the dataset shows very high imbalance between the target labels (~13:1), we needed an estimator that could focus more on classifying the minority class more accurately. In other words, the weights of the features should be adjusted in such a way that equivalent samples of minority and majority samples are classified correctly. RandomForestClassifier provides a hyperparamter \"class_weight\" which can be set with option \"balanced\". This ensures the balanced classification of majority and minority classes. In the longer run, this gave better set of features that in turn produced the best results for us.\n",
    "\n",
    "## 3. Filter training data\n",
    "In this step, training data is filtered based on selected features from previous step.\n",
    "\n",
    "## 4. Under sampling\n",
    "In this approach we randomly remove samples of the majority class (no-churn cases) in order to reduce the imbalance between the classes in the dataset. For this purpose we use python's <b>imbalanced-learn</b> library - http://imbalanced-learn.org/en/stable/. We choose the minority:majority class ratio as 1:3 or in other words, for every minority class sample, there are 3 majority class samples. Other ratios were tried as well (10:1, 5:1, 2:1 and 1:1), but 3:1 seems to have the better results of all. Here is the training set configuration:\n",
    "\n",
    "<b><u>Origial Data</u></b>\n",
    "<table align=\"left\">\n",
    "    <tr>\n",
    "        <td align=\"left\">Total training rows</td>\n",
    "        <td>25000 (100%)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td align=\"left\">Rows with outcome as \"no-churn\"</td>\n",
    "        <td>23155 (92.62%)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td align=\"left\">Rows with outcome as \"churn\"</td>\n",
    "        <td>1845 (7.38%)</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<br><b><u>Under-sampled Data</u></b>\n",
    "<table align=\"left\">\n",
    "    <tr>\n",
    "        <td align=\"left\">Total training rows</td>\n",
    "        <td>7435 (100%)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td align=\"left\">Rows with outcome as \"no-churn\"</td>\n",
    "        <td>5590 (75.18%)</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td align=\"left\">Rows with outcome as \"churn\"</td>\n",
    "        <td>1845 (24.82%)</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "<br><br><br><br><br><b><u>NOTE:</u></b>\n",
    "<br>We tried other oversampling / undersampling and combined sampling approaches using SMOTE, Tomek, Edited Nearest Neighbours etc., but with every approach there were too many noisy samples of the minority and majority class that distorted the final results. Hence we proceeded with this approach where we do not introduce any synthetic samples, but try to reduce the shadowing effect of the majority class samples. This also produced better results that any of the other oversampling-undersampling techniques mentioned here.\n",
    "\n",
    "## 5. Training with AUTO-SKLEARN\n",
    "Under-sampled training batch is trained in auto-sklearn.\n",
    "\n",
    "In <b>auto-sklearn</b> the search space is restricted to RandomForest Classifier. There are 2 reasons behind this restriction:\n",
    "a. Training with other classifiers did not yield good results. Multiple approaches were attempted using all classifiers and batches of classifiers, but results were no better than running RandomForest alone.\n",
    "b. RFECV has shortlisted best features using the RandomForest Classifier. It then seemed more logical to continue training the models using the same estimator.\n",
    "\n",
    "Training is performed for 24hrs. Taking into account the other steps mentioned above, it may take slighlty more than 24hrs for the notebook to complete it's task. Ensemble time seems to vary at times. Hence the email functionality was included to get an indication, when the runs were completed.\n",
    "\n",
    "<b><u>NOTE: </b></u><br>When each step concludes, relevant resultant objects are stored to disk using the pickle library.\n",
    "\n",
    "Every cell in the notebook is timed (%%time), which gives an idea of the cell runtime.\n",
    "<b>--------------------------------------------------------------------------------------------------</b>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: black; font-family: 'courier new'; font-size: 1.2em\">\n",
    "\n",
    "## IMPORTED LIBRARIES\n",
    "Here is a summary of the imported modules and their purposes:\n",
    "\n",
    "### 1. Utilities\n",
    "<b>a. datetime</b>\n",
    "<br>To measure start and end time of the notebook execution. Also used to suffix timestamp to the generated log files.\n",
    "\n",
    "<b>b. shutil, glob</b>\n",
    "<br>These modules are used to maintain (delete/create) folders and log files. (AUTOML uses pSMAC for distributed processing. This is done by writing and reading data to a shared folder).\n",
    "\n",
    "<b>c. warnings</b>\n",
    "<br>To suppress warnings from the auto-sklearn functions. Warnings themselves are harmless but when the training runs for hours, these warnings tend to increase the size of the notebook and can lead to jupyter crash, if disk space is at premium.\n",
    "\n",
    "<b>d. os</b>\n",
    "<br>Writing and reading log files and pickles from disk.\n",
    "\n",
    "<b>e. pickle</b>\n",
    "<br>Writing objects to disk as binary files. These objects incldue RFECV object, parsed training files, list of columns, list of column datatypes and trained auto-sklearn model.\n",
    "\n",
    "<b>f. pandas</b>\n",
    "<br>To read raw training data from csv files into dtaframes and process it.\n",
    "\n",
    "<b>g. numpy</b>\n",
    "<br>For datatype constants.\n",
    "\n",
    "### 2. Distributed processing\n",
    "<b>a. multiprocessing</b>\n",
    "<br>For distributing the auto-sklearn training workload to multiple CPU cores.\n",
    "\n",
    "### 3. Logs via email\n",
    "<b>a. smtplib</b>\n",
    "<br>Once the notebook run is completed, this module helps to send the logs and results via email. \n",
    "    \n",
    "### 4. Metrices\n",
    "<b>a. sklearn.metrics.f1_score</b>\n",
    "<br>To evaluate the model using F1 scores of binary classes.\n",
    "\n",
    "### 5. Feature selection\n",
    "<b>a. sklearn.feature_selection.VarianceThreshold</b>\n",
    "<br>To filter features that have variance less than a defined variance threshold.\n",
    "\n",
    "<b>b. sklearn.ensemble.RandomForestClassifier</b>\n",
    "<br>RandomForestClassifier (RFC) is used as the estimator to find best ranking features via recursive feature elimination. One of the main reasons for selecting RFC is that it provides a hyperparameter to balance the weights of the classes being predicted.\n",
    "\n",
    "<b>c. sklearn.feature_selection.RFECV</b>\n",
    "<br>To find the best ranking features against f1-weighted score using RandomForestClassifier as the estimator.\n",
    "\n",
    "### 6. Auto-Sklearn\n",
    "<b>a. autosklearn.classification.AutoSklearnClassifier</b>\n",
    "<br>AUTOML for classification task.\n",
    "\n",
    "<b>b. autosklearn.constants import *</b>\n",
    "<br>Constants to be used in the AUTOML functions.\n",
    "\n",
    "<b>c. autosklearn import metrics</b>\n",
    "<br>Metrice related constans for AUTOML functions.\n",
    "\n",
    "### 7. Class Imbalance\n",
    "<b>a. imblearn.under_sampling.RandomUnderSampler</b>\n",
    "<br>To reduce the number of majority class samples and bring down the ratio of majority:minority classes from ~13:1 to ~3:1.\n",
    "\n",
    "<b>--------------------------------------------------------------------------------------------------</b>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fedora/anaconda3/lib/python3.7/site-packages/pyparsing.py:2681: FutureWarning: Possible set intersection at position 3\n",
      "  self.re = re.compile( self.reString )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-15 09:27:53.534154\n"
     ]
    }
   ],
   "source": [
    "#### Utilities\n",
    "import datetime\n",
    "import shutil\n",
    "import glob\n",
    "import warnings\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#### Distributed processing\n",
    "import multiprocessing\n",
    "\n",
    "#### Send logs via email\n",
    "import smtplib\n",
    "from os.path import basename\n",
    "from email.mime.application import MIMEApplication\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "from email.utils import COMMASPACE, formatdate\n",
    "\n",
    "#### Metrices\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#### Feature selection modules\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "#### Automl related modules\n",
    "from autosklearn.classification import AutoSklearnClassifier\n",
    "from autosklearn.constants import *\n",
    "from autosklearn import metrics as amet\n",
    "\n",
    "#### Class imbalance addressing modules\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "start = datetime.datetime.now()\n",
    "print(start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: black; font-family: 'courier new'; font-size: 1.2em\">\n",
    "\n",
    "## CONSTANTS\n",
    "Specfic details of each constant are indicated in comments below.\n",
    "<b>--------------------------------------------------------------------------------------------------</b>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 665 µs, sys: 0 ns, total: 665 µs\n",
      "Wall time: 671 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#### Constant values\n",
    "\n",
    "#### Data location on the disk - absolute path of unzipped datafiles\n",
    "data_dir           = \"../unzipped_datafiles/\"\n",
    "\n",
    "#### File prefix for training and label files\n",
    "data_file_prefix   = \"orange_large_train.data.chunk\"   # training file prefix\n",
    "target_file        = \"orange_large_train_churn.labels\" # label file prefix\n",
    "\n",
    "#### File and columns counts\n",
    "tot_cols           = 14740 # only numeric cols\n",
    "tot_files          = 5     # 5 chunks\n",
    "\n",
    "#### Null and variance threholds\n",
    "null_val_threshold = .90\n",
    "variance_threshold = 0.9*(1-0.9) ## Variation <=10% as per sklearn's function requirement\n",
    "\n",
    "#### Variables for target label splits\n",
    "targ_start         = 0\n",
    "num_rows_for_targ  = 25000\n",
    "\n",
    "#### List of columns indices to process.\n",
    "col_range          = [x for x in range(0, tot_cols)]\n",
    "\n",
    "#### Num of CPU cores to use for parallel processing\n",
    "num_cores_to_use = 7 ## = 7 parallel processes\n",
    "\n",
    "#### Parameters for auto-sklearn\n",
    "estimator_for_automl = [\"random_forest\", ]  # Estimator is limited to RandomForest, since RFECV has\n",
    "                                            # selected best features using this estimator\n",
    "    \n",
    "train_duration       = 86400 # 24hrs - This is for one batch.\n",
    "ensemble_duration    = 43200 # 12hrs - This is for one batch, although in practice, ensemble requires only few mins.\n",
    "mem_to_use           = 18432 # 18GB RAM per CPU process.\n",
    "\n",
    "#### Temporary folders for auto-sklearn output. \n",
    "#### Used for building ensemble after fitting training data.\n",
    "tmp_folder         = \"../tmp/autosklearn_parallel_code5_tmp\"\n",
    "output_folder      = \"../tmp/autosklearn_parallel_code5_out\"\n",
    "dataset_name       = \"kdd_ds\"\n",
    "\n",
    "#### Variables to hold column names and column data type dictionary.\n",
    "cols_to_drop        = []\n",
    "col_names           = []\n",
    "col_type_dict       = {}\n",
    "int_list            = [np.int8, np.int16, np.int32, np.int64]\n",
    "float_list          = [np.float16, np.float32, np.float64]\n",
    "\n",
    "#### Log and result file prefixes and suffixes\n",
    "time_suffix = str(start)\n",
    "for ch in [\" \", \":\", \"-\"]:\n",
    "    time_suffix = time_suffix.replace(ch, \"\")\n",
    "\n",
    "feat_extract_txt   = \"kdd2009_\"\n",
    "f_name_prefix = \"notebook_1_\" + feat_extract_txt\n",
    "f_name_suffix = time_suffix + \".txt\"\n",
    "temp_log_str  = \"temp_log_\"\n",
    "f_res_name    = f_name_prefix + \"training_log_\" + f_name_suffix\n",
    "res_file_list = []\n",
    "\n",
    "#### Pickle constants for files that are stored to disk.\n",
    "pkl_cols_to_retain  = \"../pickles/cols_to_retain.pkl\"\n",
    "pkl_col_type_dict   = \"../pickles/col_type_dict.pkl\"\n",
    "pkl_automl_model    = \"../pickles/automl_model.pkl.0\"\n",
    "pkl_rfecv_model     = \"../pickles/rfecv_model.pkl\"\n",
    "pkl_xtrain_orig     = \"../pickles/X_train.pkl\"\n",
    "pkl_ytrain_orig     = \"../pickles/y_train.pkl\"\n",
    "\n",
    "#### Set this boolean to TRUE, to load data from disk\n",
    "#### If this boolean is set to FALSE, Parsing, RFECV based feature eliminiation etc are run again.\n",
    "#### NOTE: New run may not be guarantee better prediction results, since the random state is not preserved.\n",
    "use_saved_data = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: black; font-family: 'courier new'; font-size: 1.2em\">\n",
    "\n",
    "## Function: SEND_EMAIL\n",
    "This function uses the email credentials from a stored file and sends logs to the recepient's email id. Format of data in the \"email_cred.txt\" should be as below:\n",
    "\n",
    "from_email_id\n",
    "<br>nsn-intra password\n",
    "<br>to_email_id\n",
    " \n",
    "<b>--------------------------------------------------------------------------------------------------</b>\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1e+03 ns, total: 4 µs\n",
      "Wall time: 5.48 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def send_email (file_list):\n",
    "    \n",
    "    with open (\"../email_cred.txt\", \"r\") as fp:\n",
    "        user  = fp.readline().replace(\"\\n\", \"\")\n",
    "        pwd   = fp.readline().replace(\"\\n\", \"\")\n",
    "        recep = fp.readline().replace(\"\\n\", \"\")\n",
    "        \n",
    "    #### Add headers\n",
    "    msg = MIMEMultipart()\n",
    "    msg['From'] = user\n",
    "    msg['To'] = recep\n",
    "    msg['Subject'] = \"AUTOML Training Results Are Ready.\"\n",
    "\n",
    "    #### Add mail text\n",
    "    msg.attach(MIMEText(\"Hello,\\n\\nTraining results of batches are in attached files.\\n\\nBest Regards\"))\n",
    "\n",
    "    #### Attach files\n",
    "    for f in file_list:\n",
    "        with open(f, \"r\") as fil:\n",
    "            part = MIMEApplication(fil.read(),Name=basename(f))\n",
    "        part['Content-Disposition'] = 'attachment; filename=\"%s\"' % basename(f)\n",
    "        msg.attach(part)\n",
    "    \n",
    "    #### Open SMTP server and send mail\n",
    "    server = smtplib.SMTP(\"smtp.office365.com\", 587)\n",
    "    #server.set_debuglevel(True)\n",
    "    server.starttls()\n",
    "    server.login(user, pwd)\n",
    "    log_res = server.sendmail(user, recep, msg.as_string())\n",
    "    server.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: black; font-family: 'courier new'; font-size: 1.2em\">\n",
    "\n",
    "## Function: CHECK_AND_UPDATE_DTYPE\n",
    "Helper function that checks the maximum and minimum values of a feature and assigns an appropriate datatype.\n",
    "\n",
    "By default, pandas assigns the largest datatype for a numeric column such as int64 and float64 and this bloats up the dataframe size in memory. Using smaller datatypes, drastically reduces the memory footprint of the dataframes.\n",
    "<b>--------------------------------------------------------------------------------------------------</b>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 5.25 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def check_and_update_dtype(dt, mn, mx):\n",
    "    \n",
    "    if (dt == np.int64):\n",
    "        if ((mn >= np.iinfo(\"int8\").min) and (mx < np.iinfo(\"int8\").max)): ##int8\n",
    "            return np.int8\n",
    "        elif ((mn >= np.iinfo(\"int16\").min) and (mx < np.iinfo(\"int16\").max)): ##int16\n",
    "            return np.int16\n",
    "        elif ((mn >= np.iinfo(\"int32\").min) and (mx < np.iinfo(\"int32\").max)): ##int32\n",
    "            return np.int32\n",
    "    elif (dt == np.float64):\n",
    "        if ((mn >= np.finfo(\"float32\").min) and (mx < np.finfo(\"float32\").max)): ##float32\n",
    "            return np.float32\n",
    "    else:\n",
    "        return dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: black; font-family: 'courier new'; font-size: 1.2em\">\n",
    "\n",
    "## Function: PREPROCESS_TRAIN_COLUMNS\n",
    "Dataframe parsing is done in two steps. In the first step, the whole training dataset (25k rows) is parsed with the following intentions:\n",
    "1. Find features (columns) that have null values greater than defined threshold (Constant: null_val_threshold). These features will be dropped later during second parse.\n",
    "2. To find reduced datatypes appropriate for the features.\n",
    "\n",
    "Two step approach is more time and memory efficient than a single step apprach of reading the data and updating the data type on the fly. For example, analysing the actual data type of a feature and reducing the same in pandas using astype() function takes about 8secs for a single feature. This transaltes to about 19hrs for ~8500 features (reduced after null value checks are applied), with a training dataframe size of ~1.6GB. <br>The two step approach, takes less than 7mins to read the entire training data, amounting to ~360MB in memory.\n",
    "\n",
    "First step is performed in this function - <b>preprocess_train_columns()</b>.\n",
    "\n",
    "In the second step we parse the training data again armed with the information above. This enables us to store all the training data in one dataframe since the size is memory friendly. The second step is performed in function <b>extract_data()</b>.\n",
    "<b>--------------------------------------------------------------------------------------------------</b>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 5.25 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#### Find cols to be dropped. Read all the coulmns from all files and build the drop list and data dict.\n",
    "def preprocess_train_columns(num_files, data_dir, file_prefix):\n",
    "\n",
    "    #### break from this loop when 2.5 files are read.\n",
    "    break_loop = False \n",
    "    \n",
    "    for i in range(1, num_files+1):\n",
    "        if (i == 1):\n",
    "            hdr = 0\n",
    "        else:\n",
    "            hdr = None\n",
    "\n",
    "        chunk_df = pd.read_csv(data_dir+data_file_prefix+str(i), \n",
    "                               sep=\"\\t\", \n",
    "                               lineterminator=\"\\n\", \n",
    "                               header=hdr,\n",
    "                               usecols=col_range)\n",
    "        \n",
    "        #### Store column names from first file as they are not available in other chunks\n",
    "        if (i == 1):\n",
    "            col_names = [col for col in chunk_df.columns]\n",
    "        else:\n",
    "            chunk_df.columns = col_names\n",
    "            \n",
    "        #### Only 25k rows are part of the training set. Limit it here when the 3rd file is read.\n",
    "        if (i==3):\n",
    "            chunk_df = chunk_df.iloc[:5001]\n",
    "            break_loop = True\n",
    "\n",
    "        #### Drop columns with variance=0(constant value columns), \n",
    "        #### null values more than defined threshold and all categorical values.\n",
    "        cols_to_drop_in_chunk = [col for col in col_names\n",
    "                                 if (chunk_df[col].isnull().sum()/len(chunk_df) > null_val_threshold) #### Null values\n",
    "                                ]\n",
    "        \n",
    "        #### Since the null value and variance checks are performed for every chunk, the drop list \n",
    "        #### has to be updated after every file is read. This ensures we have a holistic view of the features\n",
    "        #### before they are dropped.\n",
    "        if (i == 1):\n",
    "            cols_to_drop[:] = [col for col in cols_to_drop_in_chunk]\n",
    "        else:\n",
    "            #### Only columns in both the new list and original list have to be maintained.\n",
    "            cols_to_drop[:] = list(set(cols_to_drop) & set(cols_to_drop_in_chunk))\n",
    "        \n",
    "        #### For the included columns check the min, max and dtypes and store them in a dictionary.\n",
    "        #### Later we will update this dictionary with reduced dtypes.\n",
    "        #cols_to_include = list(set(col_names) - set(cols_to_drop))\n",
    "        #cols_to_include = sorted(cols_to_include)\n",
    "\n",
    "        if (i == 1):\n",
    "            col_dt_min_max_list = [[col, chunk_df[col].dtype, chunk_df[col].min(), chunk_df[col].max()]\n",
    "                                   for col in chunk_df.columns]\n",
    "        else:\n",
    "            for col in chunk_df.columns:\n",
    "                mn = chunk_df[col].min()\n",
    "                mx = chunk_df[col].max()\n",
    "                dt = chunk_df[col].dtype\n",
    "                found = False\n",
    "\n",
    "                for colm in col_dt_min_max_list:\n",
    "                    if (col == colm[0]):\n",
    "\n",
    "                        #### pandas could read different dytpes for the same column depending on values in a chunk\n",
    "                        #### float dtype is given higher preference than int\n",
    "                        if ((dt != colm[1]) and (dt in float_list)):\n",
    "                            colm[1] = dt\n",
    "                        \n",
    "                        if  (mn < colm[2]):\n",
    "                            colm[2] = mn\n",
    "\n",
    "                        if (mx > colm[3]):\n",
    "                            colm[3] = mx\n",
    "\n",
    "                        break\n",
    "\n",
    "        if (break_loop): break\n",
    "    \n",
    "    #### Build the reduced datatype dictionary for numeric columns.\n",
    "    #### This dictionary will be used later when reading training data into a pandas dataframe - second parsing step.\n",
    "    for item in col_dt_min_max_list:\n",
    "        ind = int(item[0].replace(\"Var\", \"\")) - 1\n",
    "        col_type_dict[ind] = check_and_update_dtype(item[1], item[2], item[3])\n",
    "        \n",
    "    del chunk_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: black; font-family: 'courier new'; font-size: 1.2em\">\n",
    "\n",
    "## Function: EXTRACT_DATA\n",
    "This is the second step in parsing training data. In here, the training data is read from respective chunks based on reduced feature set and reduced datatype of the selected features.\n",
    "\n",
    "Once the data is in a single dataframe, null values are filled with 0 and further columns are reduced based on variance threshold. Features having variance less than defined threshold (Constant: variance_threshold) are dropped.\n",
    "\n",
    "<b><u>NOTE:</u></b>\n",
    "1. Various null filler approaches were tried such as \"Forward Filling\", \"Backward filling\" and \"Majority filling\". However null values filled with zero seems to provide better results for the selected model.\n",
    "\n",
    "2. Variance threshold can be applied only when null values are filled. This is the reason variance threshold based filtering is delayed to second step of parsing data.\n",
    "<b>--------------------------------------------------------------------------------------------------</b>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 5.72 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def extract_train_data (num_files):\n",
    "    \n",
    "    X_train = pd.DataFrame()\n",
    "    X_test  = pd.DataFrame()\n",
    "\n",
    "    #### Cycle through columns in batches, reduce their dtype and then append to final dataframe.\n",
    "    for i in range(1, num_files+1):\n",
    "        \n",
    "        if i > 3:\n",
    "            continue\n",
    "            \n",
    "        chunk_df = pd.DataFrame()\n",
    "\n",
    "        if (i == 1):\n",
    "            hdr = 0\n",
    "        else:\n",
    "            hdr = None\n",
    "\n",
    "        chunk_df = pd.read_csv(data_dir+data_file_prefix+str(i), \n",
    "                               sep=\"\\t\", \n",
    "                               lineterminator=\"\\n\", \n",
    "                               header=hdr,\n",
    "                               usecols=col_range,\n",
    "                               dtype=col_type_dict) #### Dictionary of datatype from first parsing step.\n",
    "\n",
    "        if (i == 1):\n",
    "            col_names = [col for col in chunk_df.columns]\n",
    "        else:\n",
    "            chunk_df.columns = col_names\n",
    "        \n",
    "        chunk_df.drop(columns=cols_to_drop, inplace=True)\n",
    "        \n",
    "        #chunk_df.fillna(method=null_filler, inplace=True)\n",
    "        chunk_df.fillna(0, inplace=True)\n",
    "\n",
    "        if (i < 3):\n",
    "            X_train = X_train.append(chunk_df, ignore_index=True)\n",
    "        elif (i == 3):\n",
    "            X_train = X_train.append(chunk_df.iloc[:5001], ignore_index=True)\n",
    "    \n",
    "    #### Filter columns with low variance from train and test data\n",
    "    X_var = VarianceThreshold(threshold=variance_threshold).fit(X_train)\n",
    "    indx  = X_var.get_support(indices=True) \n",
    "    X_train = X_train[X_train.columns[indx]]\n",
    "    \n",
    "    #### Release memory - although python's garbage collector is \"lazy\" and may not release the memory immediately.\n",
    "    del chunk_df\n",
    "    \n",
    "    return (X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: black; font-family: 'courier new'; font-size: 1.2em\">\n",
    "\n",
    "## Function: SPLIT_TARGET_LABELS\n",
    "Function to read 25k target labels corresponding to the training data.\n",
    "<b>--------------------------------------------------------------------------------------------------</b>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 5.96 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def split_target_labels():\n",
    "    \n",
    "    #### Split true labels into half for trainig set\n",
    "    y_train = pd.read_csv(data_dir+target_file, \n",
    "                          header=None, \n",
    "                          squeeze=True, #### Save memory by sqeezing this into a Pandas Series\n",
    "                          skiprows=targ_start, \n",
    "                          nrows=num_rows_for_targ)\n",
    "\n",
    "    return (y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: black; font-family: 'courier new'; font-size: 1.2em\">\n",
    "\n",
    "## Function: DEL_TMP_OUT_FOLDERS\n",
    "Helper function to maintain sanity of output folders for auto-sklearn. \n",
    "\n",
    "These folders store shared data that is used by parallel processes of auto-sklearn. Folders have to be deleted before a fresh run, else the script fails.\n",
    "<b>--------------------------------------------------------------------------------------------------</b>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 5.72 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#### Check if the tmp and output folders exist and if so, delete them\n",
    "def del_tmp_out_folders():\n",
    "    for dir in [tmp_folder, output_folder]:\n",
    "        try:\n",
    "            shutil.rmtree(dir)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: black; font-family: 'courier new'; font-size: 1.2em\">\n",
    "\n",
    "## ## Pre-process Training Data\n",
    "First parse of the entire training set.\n",
    "<b>--------------------------------------------------------------------------------------------------</b>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 21s, sys: 8.77 s, total: 2min 30s\n",
      "Wall time: 2min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if (use_saved_data == False):\n",
    "    ## preprocess columns based on filter options and prepare dtype dict\n",
    "    preprocess_train_columns(num_files=tot_files, data_dir=data_dir, file_prefix=data_file_prefix)\n",
    "    pickle.dump(file=(open(pkl_col_type_dict, \"wb\")), obj=col_type_dict)\n",
    "else:\n",
    "    col_type_dict = pickle.load(file=(open(pkl_col_type_dict, \"rb\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: black; font-family: 'courier new'; font-size: 1.2em\">\n",
    "\n",
    "## Extract Training Data (this has no labels)\n",
    "Second parse of the entire training set.\n",
    "<b>--------------------------------------------------------------------------------------------------</b>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint of X_train:  0.3603520616889 GB\n",
      "Shape of X_train:  (25000, 8757)\n",
      "CPU times: user 2min 6s, sys: 7.48 s, total: 2min 14s\n",
      "Wall time: 2min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if (use_saved_data == False):\n",
    "    #### Extract training data. This has no labels.\n",
    "    X_train = extract_train_data(tot_files)\n",
    "    pickle.dump(file=open(pkl_xtrain_orig, \"wb\"), obj=X_train)\n",
    "else:\n",
    "    X_train = pickle.load(file=open(pkl_xtrain_orig, \"rb\"))\n",
    "    \n",
    "print (\"Memory footprint of X_train: \", X_train.memory_usage(deep=True).sum()/1024**3, \"GB\")\n",
    "print (\"Shape of X_train: \", X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: black; font-family: 'courier new'; font-size: 1.2em\">\n",
    "\n",
    "## Extract Target Labels\n",
    "Extract target labels corresponding to the training set above.\n",
    "<b>--------------------------------------------------------------------------------------------------</b>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint of y_train:  0.00018633902072906494 GB\n",
      "Shape of y_train:  (25000,)\n",
      "CPU times: user 5.55 ms, sys: 2.97 ms, total: 8.52 ms\n",
      "Wall time: 7.44 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#### Extract target labels\n",
    "if (use_saved_data == False):\n",
    "    y_train = split_target_labels()\n",
    "    pickle.dump(file=open(pkl_ytrain_orig, \"wb\"), obj=y_train)\n",
    "else:\n",
    "    y_train = pickle.load(file=open(pkl_ytrain_orig, \"rb\"))\n",
    "    \n",
    "print (\"Memory footprint of y_train: \", y_train.memory_usage(deep=True)/1024**3, \"GB\")\n",
    "print (\"Shape of y_train: \", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: black; font-family: 'courier new'; font-size: 1.2em\">\n",
    "\n",
    "## Reduce Feature Dimensionality with RFECV\n",
    "Scikit-learn's Recursive Feature Elimination with Cross-validation (RFECV) is used with Random Forest Classifier to reduce the feature dimensionality.\n",
    "\n",
    "Since the dataset is highly imbalanced with a ratio of <b>1:12.5</b>, we set hyperparamter <b>class_weight</b> of Random Forest classifier as <b>balanced</b>. This mode uses the target labels to automatically adjust weights inversely proportional to class frequencies in the training set as <i>n_samples / (n_classes * numpy.bincount(target))</i>.\n",
    "\n",
    "With balanced class weights, we can set the <b>scoring</b> parameter of RFECV as <b>f1_weighted</b>. This helps to rank the features based on f1 score.\n",
    "\n",
    "We have set <b>n_jobs</b> as -1 so that RFECV utilises all available CPU cores. However it seems RFECV limits the usage of CPU cores to the number of cross validation folds. Since <b>cv</b> is set as 5, only 5 CPU cores were used in this case.\n",
    "\n",
    "Nested parallelism is not supported by scikit-learn (at least in version 0.20.0). Hence <b>n_jobs</b> hyperparamter has not been used with the Random Forest classifier, but only with RFECV.\n",
    "\n",
    "<b><u>NOTE:</u></b>\n",
    "<br>We did not preserve the <b>random_state</b> of RFECV runs, hence the code below to run RFECV is commented and instead we load a pickle of an RFECV that was run earlier and provided the best results from auto-sklearn.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFECV configuration: \n",
      " RFECV(cv=5,\n",
      "   estimator=RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
      "            criterion='gini', max_depth=None, max_features='auto',\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=50, n_jobs=None, oob_score=False,\n",
      "            random_state=None, verbose=0, warm_start=False),\n",
      "   min_features_to_select=1, n_jobs=-1, scoring='f1_weighted', step=25,\n",
      "   verbose=0)\n",
      "\n",
      "Used estimator:\n",
      " RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
      "            criterion='gini', max_depth=None, max_features='auto',\n",
      "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "            min_impurity_split=None, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=50, n_jobs=None, oob_score=False,\n",
      "            random_state=None, verbose=0, warm_start=False)\n",
      "\n",
      "No: of selected features =  2907\n",
      "CPU times: user 14.6 ms, sys: 8.88 ms, total: 23.5 ms\n",
      "Wall time: 22.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#### Select features based on RFECV using Random Forest Classifier\n",
    "if use_saved_data == False:\n",
    "    classif = RandomForestClassifier(n_estimators=50, class_weight=\"balanced\", n_jobs=None)\n",
    "\n",
    "    #### Seems like RFECV uses total of 5 cores, one for each of the CV loop, even though we ask to use all cores.\n",
    "    rfecv = RFECV(estimator=classif, cv=StratifiedKFold(5), n_jobs=-1, scoring=\"f1_weighted\", step=25)\n",
    "    rfecv.fit (X_train, y_train)\n",
    "    pickle.dump(file=open(pkl_rfecv_model, \"wb\"), obj=rfecv)\n",
    "else:\n",
    "    #### Load RFECV for successful run. Unable to reproduce this since the random_state was not preserved.\n",
    "    rfecv = pickle.load(open(pkl_rfecv_model, \"rb\"))\n",
    "    \n",
    "#### Print RFECV results\n",
    "print(\"RFECV configuration: \\n\", rfecv)\n",
    "print (\"\\nUsed estimator:\\n\", rfecv.estimator_)\n",
    "print (\"\\nNo: of selected features = \", rfecv.n_features_)\n",
    "\n",
    "#### Some other paramters of RFECV\n",
    "#print (\"Support     = \", rfecv.support_)\n",
    "#print (\"Ranking     = \", rfecv.ranking_)\n",
    "#print (\"Grid_Scores = \", rfecv.grid_scores_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: black; font-family: 'courier new'; font-size: 1.2em\">\n",
    "\n",
    "## Filter and Drop Features from Training Dataset\n",
    "Reduce the training dataset's feature dimensions based on features selected by RFECV.\n",
    "<b>--------------------------------------------------------------------------------------------------</b>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory footprint of X_train:  0.1610257476568222 GB\n",
      "Shape of X_train:  (25000, 2907)\n",
      "CPU times: user 271 ms, sys: 182 ms, total: 454 ms\n",
      "Wall time: 454 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#### Reduce the dataframe based on selected features\n",
    "sel_ind = rfecv.get_support(indices=True)\n",
    "X_train = X_train[X_train.columns[sel_ind]]\n",
    "\n",
    "print (\"Memory footprint of X_train: \", X_train.memory_usage(deep=True).sum()/1024**3, \"GB\")\n",
    "print (\"Shape of X_train: \", X_train.shape)\n",
    "\n",
    "pkl_file = pkl_xtrain_orig + \"_reduced\"\n",
    "pickle.dump(file=open(pkl_file, \"wb\"), obj=X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: black; font-family: 'courier new'; font-size: 1.2em\">\n",
    "\n",
    "## Random Under-Sampler\n",
    "Python's imblearn package provides a mechanism to randomly undersample the majority class samples. We choose the majority:minority class ratio as 3:1 for under-sampling. This means, for every minority class sample, there are 3 majority class samples in the new training dataset. Other ratios were tried as well (10:1, 5:1, 2:1 and 1:1), but 3:1 seems to have the better results of all. \n",
    "\n",
    "Under-sampling creates a numpy array data structure which loses the column names that were available in the pandas dataframe. Although it can still be used for further training the ensemble-models, it is not very intuitive for analysis. Hence the re-sampled training data and training lables are converted back to pandas dataframes, using the datatype dictionary that was created before, during phase-1 of parsing raw data. This ensures that the size of the resultant dataframe is still memory friendly.\n",
    "<b>--------------------------------------------------------------------------------------------------</b>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train:\t (7435, 2907)\n",
      "Memory footprint of X_train:\t 0.0478891097009182 GB\n",
      "Shape of y_train:\t (7435,)\n",
      "Sample frequency of training set:\n",
      "CPU times: user 1min 53s, sys: 1.13 s, total: 1min 55s\n",
      "Wall time: 1min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "rus = RandomUnderSampler(ratio=0.33)\n",
    "\n",
    "#### Get the column names to create the DF later.\n",
    "col_names = X_train.columns\n",
    "pickle.dump(obj=col_names, file=open(pkl_cols_to_retain, \"wb\"))\n",
    "\n",
    "X_resamp, y_resamp = rus.fit_sample(X_train, y_train)\n",
    "\n",
    "#### Build the dtypes for selected features\n",
    "col_dict = {}\n",
    "for item in col_type_dict:\n",
    "    col_dict[\"Var\"+str(item+1)] = col_type_dict[item]\n",
    "    \n",
    "X_train = pd.DataFrame()\n",
    "for ind in range (0, X_resamp.shape[1]):\n",
    "    #print(\"Started...\", ind, X_resamp[:, ind].dtype, end=\"\")\n",
    "    X_train = X_train.join(pd.DataFrame(X_resamp[:, ind], \n",
    "                                        columns=[col_names[ind]], \n",
    "                                        dtype=col_dict[col_names[ind]]), \n",
    "                           how=\"right\")\n",
    "    #print(\" - Completed.....\", col_names[ind], X_train[col_names[ind]].dtype)\n",
    "\n",
    "y_train = pd.Series(y_resamp)\n",
    "\n",
    "print(\"Shape of X_train:\\t\", X_train.shape)\n",
    "print(\"Memory footprint of X_train:\\t\", X_train.memory_usage(deep=True).sum()/1024**3, \"GB\")\n",
    "\n",
    "print(\"Shape of y_train:\\t\", y_train.shape)\n",
    "print(\"Sample frequency of training set:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: black; font-family: 'courier new'; font-size: 1.2em\">\n",
    "\n",
    "## Function: GET_PROCESS_SPAWN\n",
    "Helper function which returns a worker process, that runs an instance of auto-sklearn.\n",
    "\n",
    "Multiporcessing support in auto-sklearn is based on pSMAC algorithm, which essentially specifies how parallel processes write shared data to the disk and how different processes read from this shared data.\n",
    "\n",
    "We chose not to utilize the preprocessing functionalities of auto-sklearn (such as min-max scaling) since they tend disregard the outliers degrading the final results.\n",
    "\n",
    "We have chosen a single classifier and let auto-sklearn choose an ensemble of this classifier with differently tuned hyperparameters.\n",
    "<b>--------------------------------------------------------------------------------------------------</b>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 7.87 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "## Parallel processing functions based on SMAC implementation of auto-sklearn\n",
    "def get_process_spawn (X_train, y_train, count):\n",
    "    def spawned_process(seed, dataset_name):\n",
    "        \n",
    "        if (seed==0):\n",
    "            init_config = 25\n",
    "            smac_args   = {} \n",
    "        else:\n",
    "            init_config = 0\n",
    "            smac_args   = {\"initial_incumbent\": \"RANDOM\"}\n",
    "\n",
    "        #### Shared mode is set as true and shared folders are prevented from being deleted after every process has run\n",
    "        #### Ensembling is done in a later step and hence set ot 0 here.\n",
    "        #### Computation resource counts have been defined in the Constants section above.\n",
    "        automl = AutoSklearnClassifier(initial_configurations_via_metalearning=init_config,\n",
    "                                       ml_memory_limit=mem_to_use,\n",
    "                                       time_left_for_this_task=train_duration,\n",
    "                                       include_preprocessors=[\"no_preprocessing\"], ## We have done this when parsing data\n",
    "                                       include_estimators=estimator_for_automl, \n",
    "                                       exclude_estimators=None,\n",
    "                                       shared_mode=True,\n",
    "                                       tmp_folder=tmp_folder,\n",
    "                                       output_folder=output_folder,\n",
    "                                       delete_tmp_folder_after_terminate=False,\n",
    "                                       ensemble_size=0,\n",
    "                                       seed=seed,\n",
    "                                       smac_scenario_args=smac_args\n",
    "                                      )\n",
    "        \n",
    "        #### Fit the training data using scoring method as ROC_AUC.\n",
    "        automl.fit(X_train, y_train, dataset_name=dataset_name, metric=amet.roc_auc)\n",
    "        \n",
    "        #### Log file to store training results\n",
    "        f_name = f_name_prefix + temp_log_str + str(seed) + f_name_suffix + \"_\" + str(count)\n",
    "        \n",
    "        with open (f_name, \"a\") as ar:\n",
    "            ar.write(\"\\n\\n------------------AUTOML_SEED: \" + str(seed) + \"------------------------\")\n",
    "            ar.write (\"\\n\\n-----------------------cv_results--------------------\\n\\n\")\n",
    "            for item in automl.cv_results_:\n",
    "                ar.write (str (item) + \"\\n\")\n",
    "\n",
    "            ar.write (\"\\n\\n--------------sprint_statistics-----------------------\\n\\n\")\n",
    "            ar.write (str (automl.sprint_statistics()) + \"\\n\")\n",
    "\n",
    "    return spawned_process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: black; font-family: 'courier new'; font-size: 1.2em\">\n",
    "\n",
    "## Function: LAUNCH_PROCESSES\n",
    "Similar to _main_, this calls the helper function above multiple times to launch auto-sklearn worker processes.\n",
    "\n",
    "Each worker process is given a different seed number. The number of worker processes ca be less than or equal to the number of available CPU cores. However each process utilizes the same amount of memory, so its not possible to use all cores. Based on various CPU-memory balancing ratios, appropriate number of parallel processes and memory is defined in the CONSTANTS section above.\n",
    "\n",
    "The function join() ensures that all processes are run before the ensemble is built. Scoring function used for ensemble is ROC_AUC.\n",
    "\n",
    "No: of CPU cores and memory directly affects the number of algorithms that auto-sklearn selects for training. More the better!\n",
    "\n",
    "<b>--------------------------------------------------------------------------------------------------</b>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15 µs, sys: 0 ns, total: 15 µs\n",
      "Wall time: 317 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def launch_processes(X_train, y_train, count):    \n",
    "    \n",
    "    #### Delete tmp and output folders used for pSMAC\n",
    "    del_tmp_out_folders()\n",
    "\n",
    "    #### List of parallel processes\n",
    "    processes = []\n",
    "    spawn_process = get_process_spawn(X_train, y_train, count)\n",
    "\n",
    "    #### Launch the worker processes\n",
    "    for i in range (num_cores_to_use):\n",
    "        p = multiprocessing.Process(target=spawn_process, args=(i, dataset_name))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "\n",
    "    #### Wait for worker processes to complete.\n",
    "    for proc in processes:\n",
    "        proc.join()\n",
    "\n",
    "    #### Build the ensemble of models here using data from shared folders.\n",
    "    automl = AutoSklearnClassifier(initial_configurations_via_metalearning=0,\n",
    "                                   ml_memory_limit=mem_to_use,\n",
    "                                   time_left_for_this_task=ensemble_duration,\n",
    "                                   shared_mode=True,\n",
    "                                   tmp_folder=tmp_folder,\n",
    "                                   output_folder=output_folder,\n",
    "                                   ensemble_size=50,\n",
    "                                   ensemble_nbest=50,\n",
    "                                   seed=1\n",
    "                                   )\n",
    "    \n",
    "    automl.fit_ensemble(y_train, \n",
    "                        task=BINARY_CLASSIFICATION,\n",
    "                        metric=amet.roc_auc,\n",
    "                        precision=\"64\",\n",
    "                        dataset_name=dataset_name)\n",
    "    \n",
    "    #### Store the model to disk.\n",
    "    pickle.dump(file=open(pkl_automl_model+\".\"+str(count), \"wb\"), obj=automl)\n",
    "    \n",
    "    \n",
    "    #### Log file of final ensemble.\n",
    "    with open(f_res_name + \"_\" + str(count), \"a\") as ar:\n",
    "            \n",
    "        for file in os.listdir():\n",
    "            if ((f_name_prefix+temp_log_str) in file):\n",
    "                with open(file, \"r\") as fp:\n",
    "                    ar.write (fp.read())\n",
    "                    ar.write(\"\\n\")\n",
    "                os.remove(file)\n",
    "            \n",
    "        ar.write (\"\\n\\n------------------show_models----------------------------\\n\\n\")\n",
    "        ar.write (str (automl.show_models()) + \"\\n\")    \n",
    "\n",
    "        ar.write (\"\\n\\n------------------params----------------------------\\n\\n\")\n",
    "        ar.write (str (automl.get_params()) + \"\\n\")\n",
    "    \n",
    "    #### Append to result file list to be emailed later.\n",
    "    res_file_list.append(f_res_name + \"_\" + str(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: black; font-family: 'courier new'; font-size: 1.2em\">\n",
    "\n",
    "## Function: PRINT_ENSEMBLE_DETAILS\n",
    "Helper function that prints the details of the selected model, such as the hyperparameters, weights etc.\n",
    "<b>--------------------------------------------------------------------------------------------------</b>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 6.91 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#### Get raw details of selected ensemble. Each algo+hyperparamter combination is stored as a dictionary\n",
    "def print_ensemble_details (mod_with_wt):\n",
    "\n",
    "    #### Build list of the ensemble detail dictionary\n",
    "    dict_list = []\n",
    "    for item in mod_with_wt:\n",
    "        co_dict   = {}\n",
    "        pip = item[1]\n",
    "        co_dict = pip.configuration.get_dictionary().copy()\n",
    "        co_dict[\"algo_weight\"] = item[0]\n",
    "        dict_list.append(co_dict)\n",
    "\n",
    "    #### Build a dictionary with the key is the hyperparamter name and value is a list corresponding to the hyperparamter\n",
    "    #### values of the ensemble in that order.\n",
    "    print_dict = {}\n",
    "    for item in dict_list:\n",
    "        for key, val in item.items():\n",
    "            if key in print_dict.keys():\n",
    "                pass\n",
    "            else:\n",
    "                print_dict[key] = []\n",
    "\n",
    "    for item in dict_list:\n",
    "        for key in print_dict.keys():\n",
    "            if key in item.keys():\n",
    "                print_dict[key].append(item[key])\n",
    "            else:\n",
    "                print_dict[key].append(\"NA\")\n",
    "\n",
    "    #### Read the dictionary into a pandas dataframe which is easier to print as a table in the end.\n",
    "    print_df = pd.DataFrame(print_dict)\n",
    "    col_dict = {}\n",
    "    drop_list  = []\n",
    "    const_dict = {}\n",
    "\n",
    "    for col in print_df.columns:\n",
    "\n",
    "        #### Remove parameters that are not relevant. For e.g. \n",
    "        #### 1. There are no categorical columns in our dataset\n",
    "        #### 2. There is no preprocessing / imputation done becasue the null values are filled before training the automl.\n",
    "        if ((\"categorical\" in col) or (\"preprocessor\" in col) or (\"imputation\" in col)) :\n",
    "            drop_list.append(col)\n",
    "        else:\n",
    "            str1 = col.split(\":\")\n",
    "\n",
    "            if (len(str1) > 2):\n",
    "                title = str1[2]\n",
    "            else:\n",
    "                title = str1[0]\n",
    "\n",
    "            #### Seprate parameters that have constant values for the ensemble.\n",
    "            if (len(print_df[col].unique()) == 1):\n",
    "                val = print_df[col].unique()[0]\n",
    "                if (val == \"None\" or val == 0):\n",
    "                    pass\n",
    "                else:\n",
    "                    const_dict[title] = val\n",
    "                drop_list.append(col)\n",
    "            else:\n",
    "                col_dict[col] = title\n",
    "\n",
    "    print_df = print_df.drop(drop_list, axis=1)\n",
    "    print_df = print_df.rename(col_dict, axis=1)\n",
    "\n",
    "    print(\"ENSEMBLE Constants\")\n",
    "    print(\"------------------\")\n",
    "    for k, v in const_dict.items():\n",
    "        print (k, \"\\t= \", v)\n",
    "\n",
    "    print(\"\\n\\nENSEMBLE Hyperparameters\")\n",
    "    print(\"------------------------\")\n",
    "\n",
    "    print_df.index = np.arange(1, len(print_df)+1)\n",
    "    \n",
    "    return print_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: black; font-family: 'courier new'; font-size: 1.2em\">\n",
    "\n",
    "## Train and ensemble Under-sampled Training Batch\n",
    "\n",
    "<b>--------------------------------------------------------------------------------------------------</b>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] [2018-11-15 09:43:51,945:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2018-11-15 09:43:51,945:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2018-11-15 12:54:25,792:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2018-11-15 12:54:25,792:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2018-11-15 12:58:54,759:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2018-11-15 12:58:54,759:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2018-11-15 13:27:50,734:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2018-11-15 13:27:50,734:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2018-11-15 14:06:42,599:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2018-11-15 14:06:42,599:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2018-11-15 17:39:29,592:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2018-11-15 17:39:29,592:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "CPU times: user 15.2 s, sys: 2.91 s, total: 18.1 s\n",
      "Wall time: 1d 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    count=0\n",
    "    launch_processes(X_train, y_train, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: black; font-family: 'courier new'; font-size: 1.2em\">\n",
    "\n",
    "# SUMMARY\n",
    "After successful training, we can see that auto-sklearn has successfully run <b>~5500</b> of algorithms and selected an ensemble of RandomForest that can best predict the churn.\n",
    "\n",
    "## Model Ensemble Description\n",
    "Auto-sklearn builds an ensemble of models that best classify the dataset. We have restricted the classifier selection to RandomForest, since this seems to provide the best results until now. Auto-sklearn then builds an ensemble of RandomForest Classifier models with varying hyperparameter values. The models in ensemble are shown below along with their selected hyper parameters that provided the best scoring value, which in our case is ROC_AUC. \n",
    "\n",
    "For example, ensemble below, contains 19 RandomForest Classifier models and here is a brief description of the columns:\n",
    "\n",
    "1. <b>balancing</b>\n",
    "This describes if the samples are weighted in the model.\n",
    "\n",
    "2. <b>rescaling</b>\n",
    "This describes how the feature values were scaled.\n",
    "\n",
    "3. <b>bootstrap</b>\n",
    "This describes if the boostrapping based resampling was performed during training. Basically this means that a sample of dataset is used from the training dataset for each training iteration and its possible that a sample appears multiple times in different iterations.\n",
    "\n",
    "4. <b>criterion</b>\n",
    "In tree based classification, there are two criteria used for spltting at a node - <b>gini</b> that reduces the probability of mis-classification at a node and <b>entropy</b> that reduces the impurity of classification at a node.\n",
    "\n",
    "5. <b>max_features</b>\n",
    "It seems auto-sklearn applies it's own feature dimensionality reduction techniques. Hence every model seems to be using anly a fraction of the total available features.\n",
    "\n",
    "6. <b>min_samples_leaf</b>\n",
    "Minimum no: of samples at a leaf node of the tree.\n",
    "\n",
    "7. <b>min_samples_split</b>\n",
    "Minimum no: of samples at a node, before it is split into branches.\n",
    "\n",
    "8. <b>algo_weight</b>\n",
    "This describes the weight of the predictions that the model contributes to the ensemble. For instance 0.18 indciates that 18% of the perdiction weight comes from this model. 0.14 indicates 14% of the prediction weight comes from the second model and so on.\n",
    "\n",
    "9. <b>q_max, q_min, n_quantiles and output_distrubtion</b>\n",
    "This describes the values based on selected scaling methods in each model of the ensemble.\n",
    "<b>--------------------------------------------------------------------------------------------------</b>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log files found:  1\n",
      "Number of target algorithm runs:  6068\n",
      "Number of successful target algorithm runs:  5538\n",
      "\n",
      "\n",
      "========================\n",
      "*****MODEL-ENSEMBLE*****\n",
      "========================\n",
      "ENSEMBLE Constants\n",
      "------------------\n",
      "classifier \t=  random_forest\n",
      "n_estimators \t=  100\n",
      "\n",
      "\n",
      "ENSEMBLE Hyperparameters\n",
      "------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>balancing</th>\n",
       "      <th>rescaling</th>\n",
       "      <th>bootstrap</th>\n",
       "      <th>criterion</th>\n",
       "      <th>max_features</th>\n",
       "      <th>min_samples_leaf</th>\n",
       "      <th>min_samples_split</th>\n",
       "      <th>algo_weight</th>\n",
       "      <th>n_quantiles</th>\n",
       "      <th>output_distribution</th>\n",
       "      <th>q_max</th>\n",
       "      <th>q_min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>none</td>\n",
       "      <td>standardize</td>\n",
       "      <td>False</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.844016</td>\n",
       "      <td>20</td>\n",
       "      <td>18</td>\n",
       "      <td>0.18</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>weighting</td>\n",
       "      <td>none</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.881737</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>0.14</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>weighting</td>\n",
       "      <td>quantile_transformer</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.849415</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.12</td>\n",
       "      <td>116</td>\n",
       "      <td>normal</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>weighting</td>\n",
       "      <td>none</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.833178</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>0.10</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>weighting</td>\n",
       "      <td>none</td>\n",
       "      <td>True</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.901453</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>0.06</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>weighting</td>\n",
       "      <td>quantile_transformer</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.970101</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>0.06</td>\n",
       "      <td>130</td>\n",
       "      <td>uniform</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>weighting</td>\n",
       "      <td>none</td>\n",
       "      <td>True</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.901453</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>0.04</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>weighting</td>\n",
       "      <td>robust_scaler</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.907332</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>0.04</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0.92252</td>\n",
       "      <td>0.286795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>weighting</td>\n",
       "      <td>none</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.975166</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>0.04</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>weighting</td>\n",
       "      <td>quantile_transformer</td>\n",
       "      <td>True</td>\n",
       "      <td>gini</td>\n",
       "      <td>0.971916</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "      <td>0.04</td>\n",
       "      <td>157</td>\n",
       "      <td>normal</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>weighting</td>\n",
       "      <td>quantile_transformer</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.970101</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>0.02</td>\n",
       "      <td>644</td>\n",
       "      <td>uniform</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>weighting</td>\n",
       "      <td>quantile_transformer</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.970101</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>0.02</td>\n",
       "      <td>905</td>\n",
       "      <td>uniform</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>weighting</td>\n",
       "      <td>robust_scaler</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.907332</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0.830663</td>\n",
       "      <td>0.25349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>weighting</td>\n",
       "      <td>minmax</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.970989</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>weighting</td>\n",
       "      <td>minmax</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.970989</td>\n",
       "      <td>17</td>\n",
       "      <td>13</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>weighting</td>\n",
       "      <td>minmax</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.970989</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>weighting</td>\n",
       "      <td>robust_scaler</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.907332</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>0.92252</td>\n",
       "      <td>0.286795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>weighting</td>\n",
       "      <td>standardize</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.912047</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>0.02</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>weighting</td>\n",
       "      <td>quantile_transformer</td>\n",
       "      <td>True</td>\n",
       "      <td>entropy</td>\n",
       "      <td>0.849415</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0.02</td>\n",
       "      <td>116</td>\n",
       "      <td>normal</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    balancing             rescaling bootstrap criterion  max_features  \\\n",
       "1        none           standardize     False   entropy      0.844016   \n",
       "2   weighting                  none      True   entropy      0.881737   \n",
       "3   weighting  quantile_transformer      True   entropy      0.849415   \n",
       "4   weighting                  none      True   entropy      0.833178   \n",
       "5   weighting                  none      True      gini      0.901453   \n",
       "6   weighting  quantile_transformer      True   entropy      0.970101   \n",
       "7   weighting                  none      True      gini      0.901453   \n",
       "8   weighting         robust_scaler      True   entropy      0.907332   \n",
       "9   weighting                  none      True   entropy      0.975166   \n",
       "10  weighting  quantile_transformer      True      gini      0.971916   \n",
       "11  weighting  quantile_transformer      True   entropy      0.970101   \n",
       "12  weighting  quantile_transformer      True   entropy      0.970101   \n",
       "13  weighting         robust_scaler      True   entropy      0.907332   \n",
       "14  weighting                minmax      True   entropy      0.970989   \n",
       "15  weighting                minmax      True   entropy      0.970989   \n",
       "16  weighting                minmax      True   entropy      0.970989   \n",
       "17  weighting         robust_scaler      True   entropy      0.907332   \n",
       "18  weighting           standardize      True   entropy      0.912047   \n",
       "19  weighting  quantile_transformer      True   entropy      0.849415   \n",
       "\n",
       "    min_samples_leaf  min_samples_split  algo_weight n_quantiles  \\\n",
       "1                 20                 18         0.18          NA   \n",
       "2                  7                  9         0.14          NA   \n",
       "3                 10                  3         0.12         116   \n",
       "4                 15                 13         0.10          NA   \n",
       "5                 19                  3         0.06          NA   \n",
       "6                 17                  9         0.06         130   \n",
       "7                 19                  3         0.04          NA   \n",
       "8                 19                  2         0.04          NA   \n",
       "9                 15                 19         0.04          NA   \n",
       "10                17                 11         0.04         157   \n",
       "11                17                  6         0.02         644   \n",
       "12                17                  5         0.02         905   \n",
       "13                19                  2         0.02          NA   \n",
       "14                17                  9         0.02          NA   \n",
       "15                17                 13         0.02          NA   \n",
       "16                17                 16         0.02          NA   \n",
       "17                20                  2         0.02          NA   \n",
       "18                19                 20         0.02          NA   \n",
       "19                10                  3         0.02         116   \n",
       "\n",
       "   output_distribution     q_max     q_min  \n",
       "1                   NA        NA        NA  \n",
       "2                   NA        NA        NA  \n",
       "3               normal        NA        NA  \n",
       "4                   NA        NA        NA  \n",
       "5                   NA        NA        NA  \n",
       "6              uniform        NA        NA  \n",
       "7                   NA        NA        NA  \n",
       "8                   NA   0.92252  0.286795  \n",
       "9                   NA        NA        NA  \n",
       "10              normal        NA        NA  \n",
       "11             uniform        NA        NA  \n",
       "12             uniform        NA        NA  \n",
       "13                  NA  0.830663   0.25349  \n",
       "14                  NA        NA        NA  \n",
       "15                  NA        NA        NA  \n",
       "16                  NA        NA        NA  \n",
       "17                  NA   0.92252  0.286795  \n",
       "18                  NA        NA        NA  \n",
       "19              normal        NA        NA  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "CPU times: user 4.97 s, sys: 1.3 s, total: 6.27 s\n",
      "Wall time: 6.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#### Copy files to a common dir\n",
    "file_list = glob.glob(\"*.txt*\")\n",
    "for file in file_list:\n",
    "    shutil.copy(file, \"../pickles/.\")\n",
    "\n",
    "target_alog_str = \"Number of target algorithm runs: \"\n",
    "succes_algo_str = \"Number of successful target algorithm runs: \"\n",
    "\n",
    "target_algos = 0\n",
    "succes_algos = 0\n",
    "file_count = 0\n",
    "log_dir = \"../pickles/\"\n",
    "for file in os.listdir(log_dir):\n",
    "    \n",
    "    if f_res_name in file:\n",
    "        file_count += 1\n",
    "        with open(log_dir+file, \"r\", encoding=\"utf-8\") as fp:\n",
    "            for line in fp:\n",
    "                line = line.rstrip()\n",
    "                if target_alog_str in line:\n",
    "                    tokens = line.split()\n",
    "                    target_algos += int(tokens[-1])\n",
    "                elif succes_algo_str in line:\n",
    "                    tokens = line.split()\n",
    "                    succes_algos += int(tokens[-1])\n",
    "\n",
    "print (\"Log files found: \", file_count)\n",
    "print (target_alog_str, target_algos)\n",
    "print (succes_algo_str, succes_algos)\n",
    "\n",
    "#### Print model ensemble details\n",
    "with open(pkl_automl_model, \"rb\") as fp:\n",
    "    model = pickle.load(file=fp)\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"========================\")\n",
    "    print(\"*****MODEL-ENSEMBLE*****\")\n",
    "    print(\"========================\")\n",
    "\n",
    "    display(print_ensemble_details(model.get_models_with_weights()))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: black; font-family: 'courier new'; font-size: 1.2em\">\n",
    "\n",
    "## Send email with log files\n",
    "\n",
    "<b>--------------------------------------------------------------------------------------------------</b>\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mail sent with logs .....\n",
      "CPU times: user 14.7 ms, sys: 3.08 ms, total: 17.8 ms\n",
      "Wall time: 2.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "send_email (res_file_list)\n",
    "print(\"Mail sent with logs .....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-16 09:43:54.244007\n",
      "1 day, 0:16:00.709853\n"
     ]
    }
   ],
   "source": [
    "end = datetime.datetime.now()\n",
    "print(end)\n",
    "print(end-start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
